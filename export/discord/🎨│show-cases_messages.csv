Author,Content,Timestamp
RexiaAI,"https://www.linkedin.com/pulse/rexiaai-pioneering-agentic-development-local-ai-models-robyn-le-sueur-qyayf/?trackingId=chSLYRk8Tz6%2BWd5GCo8jUA%3D%3D

Someone may find this useful:  it's an agentic framework designed to work with local LLM's like Yi 1.5 9b, but it also works rather well with models like Yi-Large. It provides function calling with models that don't generally support it by using structured prompting techniques.",2024-06-26 13:18:03.847000+00:00
Yu@01.ai,<@126314401388036096> Congrats on the awesome project! Agent is one of the hottest directions now. Super excited to see Yi models being integrated into a wider ecosystem! üòÄ,2024-06-26 14:01:41.230000+00:00
RexiaAI,"If it's of use to anyone, Yi-Large functions very well in a 'LLM as a tool' role, where by a smaller local model can call a larger, more powerful model for complex tasks.

As shown here: 
https://www.linkedin.com/pulse/creating-tools-rexiaai-comprehensive-guide-robyn-le-sueur-anxsf/",2024-06-27 12:24:12.364000+00:00
Yu@01.ai,<@126314401388036096> Cool! This is a great RAG example with function call capability built on the Rexia agent (and we've found that the Rexia agent framework has been added more features). Looking forward to seeing more agent use cases! üòÄ,2024-06-27 13:25:24.815000+00:00
Tonic,"hey there, check out the new technique (in python) for using yi with testcontainers : 

```python
from json import loads
from pathlib import Path
from requests import post
from testcontainers.ollama import OllamaContainer

def split_by_line(generator):
    data = b''
    for each_item in generator:
        for line in each_item.splitlines(True):
            data += line
            if data.endswith((b'\r\r', b'\n\n', b'\r\n\r\n', b'\n')):
                yield from data.splitlines()
                data = b''
    if data:
        yield from data.splitlines()

def main():
    with OllamaContainer(ollama_home=Path.home() / "".ollama"") as ollama:
        # List available models
        models = ollama.list_models()
        print(""Available models:"", models)

        # Choose a specific model, for example 'llama3:latest'
        model_name = ""yi:6b-v1.5""
        if model_name not in [model[""name""] for model in models]:
            print(f""Model '{model_name}' not found, pulling the model."")
            ollama.pull_model(model_name)
        
        # Use the model to generate a response
        endpoint = ollama.get_endpoint()
        response = post(
            url=f""{endpoint}/api/chat"", 
            stream=True, 
            json={
                ""model"": model_name,
                ""messages"": [{
                    ""role"": ""user"",
                    ""content"": ""What color is the sky? MAX ONE WORD""
                }]
            }
        )

        for chunk in split_by_line(response.iter_content()):
            print(loads(chunk)[""message""][""content""], end="""")

if __name__ == ""__main__"":
    main()
```

that's it ! then you will use the yi model family with the new OllamaContainer object inside docker / testcontainers , and when you're finished it's totally gone !",2024-06-27 20:37:05.870000+00:00
Tonic,https://github.com/MultiTonic/local-llama-testcontainers,2024-06-27 22:59:32.546000+00:00
Tonic,"check out the cool demos üôÇ , going to keep improving these , soon it will be super cool with code execution and function calling , but ephemeral üôÇ",2024-06-27 23:00:16.283000+00:00
Nix,"Just finished refining my Ollama/OpenWebUI setup with models that I find work best for my use cases right now. Most powerful on top, least powerful on the bottom.

Yi is Yi-Large
Athena is LLaMA-3-70B-Instruct
Ivy is Yi-1.5-34B-Chat (been having issues with some tunes and the 16K version, still got to figure out why but not sure, maybe importing the model wrong possibly in ollama? will figure it out at some point)
Iris is Codestral-22B
Amica is a finetune of LLaMA-3-8B, thinking of switching to Yi-1.5-9B but not sure what tune would fit
Mella is Phi-3-Mini
and Lili is Qwen-2-0.5B

---

Yi = (outside of testing this month) Best performance I need for a task that Athena cannot do, when I am using both GPUs and don't want to load the model, when I want the best overall experience with a model, or I need best performance with good speed.
Athena = When I need a complex task done or I just want to chat with a model about anything with a lot of complexity
Ivy = When I want to talk to a powerful model but also need to use another GPU
Iris = When I need the best coding performance locally
Amica = When I need a unrestricted chat with a model without overloading my GPUs such as having a model on standby with good speed
Mellia = When I need a fast model for problems but don't need the best power, but speed
Lili = When I need to test out capabilities with prompting, or when I need to generate a bunch of simple things like names for example with as much speed as possible

As newer models are released and as models prove themselves to me, they will replace models within this list, but these are the models I turn to for all my local needs, as well as Yi-Large. Other than this list, the only LLM I will use now in some edge cases is Claude 3.5 Sonnet, but these models do 95% of everything I throw at them, and Yi-Large gives me the best overall performance out of the list, with only Codestral sometimes outperforming it in code, but only at times",2024-06-30 20:51:47.958000+00:00
Chen@01.ai,It's an impressive work that you find the advantage of each models to optimize your workflow. Also feel so delighted that Yi-Large can help that muchüëè,2024-07-02 06:14:12.969000+00:00
Nix,"Thanks so much! Each model has their own advantages and disadvantages in their own unique ways in terms of training data, size, speed, performance, etc, so I found it best to have a collection of models that are general but I can choose to get the best performance!

Yi-Large has been absolutely wonderful to work with, also helping me plan out a down the road project involving a newer type of chess engine

I got plans you already know for a good implementation for Yi-Large coming very soon <3",2024-07-02 06:18:33.546000+00:00
valis2400,"I've been building this time capsule with Yi-Large for a while, UI design is definitely not my biggest strength, but if you're interested in Chinese history give it a try at: https://xn--ehv518h.xyz/",2024-07-02 23:13:14.964000+00:00
valis2400,,2024-07-02 23:13:32.976000+00:00
valis2400,Any feedback is much appreciated üôÜ‚Äç‚ôÇÔ∏è,2024-07-02 23:13:45.332000+00:00
